{% extends "base.html.twig" %}

{% block title %}Metrics{% endblock %}

{% block body %}
    <article class="art-row">
        <div class="div-col-1">
            <h2>Metrics links</h2>
            <ul class="about-links">
                <li><a href="#Introduktion">Introduktion</a></li>
                <li><a href="#Phpmetrics">Phpmetrics</a></li>
                <li><a href="#Scrutinizer">Scrutinizer</a></li>
                <li><a href="#Förbättringar">Förbättringar</a></li>
                <li><a href="#Resultat">Resultat</a></li>
                <li><a href="#Diskussion">Diskussion</a></li>
            </ul>
        </div>

        <div class="div-col-2 metrics">
            <h2 id="Introduktion">Introduktion</h2>
            <p>Under detta och tidigare kursmoment har ett antal verktyg introducerats som alla är på ett eller annat sätt relaterade till begreppet "clean code". Det handlar om allt 
            ifrån hur vår kod ser ut och vart vi placerar måsvingar och parenteser till hur vi bygger upp klasser, metoder och funktioner och hur dessa används. Till dessa riktlinjer 
            tillkommer vissa mätvärden vid användandet av de verktyg som kan hjälpa oss, i det här fallet Phpmetrics och Scrutinizer. Några utav dessa mätvärden kan sammanfattas under 
            vad som kallas de sex C:na och dessa är:</p>

            <p><b>Codestyle</b></p>
            <p>Är ett bregrepp som representerar hur vi skriver våran kod, oftast handlar det om riktlinjer som vi väljer att följa inom ett specifikt språk eller på en viss arbetsplats för 
            att koden skall vara lättläslig. Att vi förbinder oss att skriva kod som följer en viss stil innebär inte bara att den blir mer lättläslig för oss själva utan även för andra och om 
            de personerna i sin tur följer samma kodstil så blir det även lättare för oss att sätta oss in i och läsa deras kod. En kodstil blir med andra ord ett 
            viktigt verktyg för att kunna arbeta tillsammans.</p>

            <p><b>Coverage</b></p>
            <p>Är ett mått på hur stora delar, eller procent av kodraderna, som är täckta utav enhetstest. En hög siffra här är dock ingen garant på att koden är bra eller att 
            den fungerar som den skall, det kan vara lätt att stirra sig blind också. Men högre täckning är generellt en indikation på att det i alla fall finns en intention att 
            testa kodbasen och det är trots allt bättre än att inte testa alls. Detta mätvärde är dessutom till viss del kopplat till andra mätvärden så som complexity och crap. Ju mer 
            komplex en klass, metod eller funktion är desto viktigare blir det att den testast ordentligt och där kan kodtäckningen spela större roll. Jag valde t.ex. att täcka mina mer 
            komplicerade metoder och klasser med fler testfall och att testa dess rader kod på fler sätt. En del av den tiden hade jag kunnat lägga på att testa andra mindre komplexa delar 
            av min kod och på så vis haft ett större värde på min coverage, men hade koden varit bättre? Hade koden varit mer vältestad och dess funktioner buggfria? Svårt att svara på men det 
            är värt att ha i beaktning vid ställningstagandet till den siffra som mätvärdet "Coverage" representerar.</p>

            <p><b>Complexity</b></p>
            <p>Är ett mätvärde som exemplifierar hur komplex koden är, hur många vägar finns det genom koden. Ju fler vägarna är desto svårare blir koden både att läsa och att underhålla. Det blir 
            dessutom svårare att felsöka och kräver både mer och mer svårskriven testkod för att arbeta mot en komplex klass/metod/funktion. Kallas även cyclomatic complexity.</p>

            <p><b>Cohesion</b></p>
            <p>Är ett mätvärde som indikerar hur väl saker som hör ihop, faktiskt hör ihop. Tanken är att saker som förändras tillsammans också bör finnas tillsammans på samma plats. 
            Det blir beroende på dess värde alltså en indikator på en klass som är fokuserad och löser enbart det den bör lösa eller så kan det indikera att en klass gör lite för mycket och 
            att den med fördel skulle kunna brytas upp i flera klasser som alla gör sin sak. Detta värde kan dyka upp under namnet LCOM(lack of cohesion of methods) och grundläggande är att lägre 
            värden indikerar mer fokuserade klasser och ett högre värde kan vara en indikation på ett en klass gör mer än den bör. Detta är även relaterad till en annan sådan princip som är användbar 
            när det kommer till "clean code", nämligen single responsibility principle, dvs en klass/modul/funktion skall bara vara ansvarig för en sak. Fokuserad kod som ansvarar för en sak var är 
            lättare att läsa och att testa!</p>

            <p><b>Coupling</b></p>
            <p>Detta är ett mätvärde som beskriver klassers koppling till varandra och kan delas upp i AC(Afferent coupling) och EC(Efferent coupling) där det förstnämnda handlar om utgående 
            kopplingar, hur många andra klasser påverkas av klassen. EC i sin tur är då inkommande kopplingar, alltså hur många klasser som påverkar just den här klassen. Dessa skillnader kan vi med 
            enkelhet exemplifiera med de värden som jag har fått på min kodbas. Jag har ju i tidigare moment fokuserat extra mycket på DI (dependency injection), dvs att vi stoppar in de klasser som 
            en klass använder sig av istället för att klassen i sig skapar dem, detta borde visa sig i mina mätvärden. Och det ser vi på min klass för spelet tjugoett, den klassen har 
            ett AC på 1 dvs den påverkar en klass. Och ett EC på 3, dvs den påverkas av tre klasser vilket rimligtvis bör vara de tre klasser vi injicerar när vi skapar ett objekt av klassen.</p>

            <p><b>CRAP</b></p>
            <p>Står för "Change risk anti-patterns och är ett mätvärde som är ett resultat av vår kods komplexitet och dess testning (coverage). Ju mer komplex kod du har och desto mer otestad 
            den är desto större CRAP-värde kommer koden att få. Värdet i sig ger oss alltså en bild av den risk för problem som existerar till följd av vår kods komplexitet och hur mycket vi testar den. 
            Med det sagt så är komplex kod inte alltid fel, ibland behövs den. Jag skriver garanterat kod som är mer komplex än den behöver vara men det finns garanterat skäl för komplex kod ibland och då 
            kan det vara värt att ta i beaktning att den koden således också bör testas väl.</p>

            <h2 id="Phpmetrics">Phpmetrics</h2>
            <p>Först och främst så innebar ju körningen av phpmetrics att jag direkt får äta en stor bit "humble pie", jag skulle vilja påstå att jag förhåller mig ödmjuk till mina förmågor att skriva 
            bra kod men att mötas av en vägg av röda bubblor direkt fullkomligen skrek "förbättringspotential". Storleken har att göra med klassens komplexitet och färgen med dess index för hur lätt 
            den blir att underhålla över tid. Detta index påverkas också av kommentarer i koden och är något jag kommer testa att förbättra. Jag ska även titta närmre på mina klassers komplexitet.</p>
            <img src="{{ asset('img/phpmetrics.jpg') }}" alt="php metrics">
            <img src="{{ asset('img/classphpmetrics.jpg') }}" alt="class metrics before">
            <p>Här ser vi även att TwentyOne och DeckOfCards är mina mest komplexa klasser, de har en del tester som körs mot dem. Hur kommer det se ut om vi kör fler tester och försöker 
            minska komplexiteten?</p>

            <h2 id="Scrutinizer">Scrutinizer</h2>
            <p>När det kommer till Scrutinizer så är det lite snällare mot egot, här ser det lite grönare och bättre ut men hur kan vi förbättra även detta? Jag tänker mig att jag definitivt 
            skall tackla de issues som scrutinizer hittat, dessutom borde kodtäckningen kunna förbättras.</p>
            <img src="{{ asset('img/scrutinizer.jpg') }}" alt="scrutinizer">
            <p>Scrutinizer visar även vilka metoder som har sämst "rating", detta kan ge oss en ledtråd att vi ska fokusera närmre på just de metoderna och försöka göra de mindre komplexa.</p>
            <img src="{{ asset('img/worstbeforescrut.jpg') }}" alt="worst class scrutinizer">
            <p>Allt verkar ju frid och fröjd när vi tittar övergripligt på scrutinizer men dyker vi närmre så finns där alltid saker att förbättra, som att göra vissa metoder mindre komplexa, 
            ändra docblock-kommentarer som scrutinizer inte förstår, ta bort död eller överflödig kod. Allt detta finns fortfarande att göra i en kodbas som scrutinizer i övrigt ger ett högt betyg.</p>
                <div class="div-row">
                    <a href="https://scrutinizer-ci.com/g/PatrikArvius/MVC/">
                    <img src="https://scrutinizer-ci.com/g/PatrikArvius/MVC/badges/quality-score.png?b=main" alt="scrutinizer score">
                    </a>
                    <a href="https://scrutinizer-ci.com/g/PatrikArvius/MVC/">
                    <img src="https://scrutinizer-ci.com/g/PatrikArvius/MVC/badges/coverage.png?b=main" alt="scrutinizer coverage">
                    </a>
                    <a href="https://scrutinizer-ci.com/g/PatrikArvius/MVC/">
                    <img src="https://scrutinizer-ci.com/g/PatrikArvius/MVC/badges/build.png?b=main" alt="scrutinizer build">
                    </a>
                </div>

            <h2 id="Förbättringar">Förbättringar</h2>
            <p>Jag har tänkt mig att göra följande förbättringar i min kod, en del i detta är givetvis en önskan om att förbättra koden i sig men det utgör också en grund i att skapa mig en större 
            förståelse för hur dessa mätverktyg fungerar.</p>
            <p>1. Öka min coverage, dvs att skriva mer enhetstester och testa större delar av min kodbas. Jag tänker att det torde vara ett relativt enkelt sätt att öka mätvärdena, dessutom 
            tänker jag försöka att inte enbart skriva tester för otestad kod utan jag vill dessutom skriva fler testfall till kod jag redan testat för att se om det får någon effekt på värdena. Det innebär 
            förstås potentiellt att själva täckningssiffran kanske inte går upp nämnvärt men det blir intressant att se.</p>
            <p>2. Fixa issues och violations som phpmetrics rapporterat. En sak som dyker upp i phpmetrics är bland annat att en klass är "probably bugged", går 
            det att få bort detta med fler tester? Jag skulle gissa att en mer gediget testad kod borde få ett lägre värde här.</p>
            <p>3. Scrutinizer och phpstan verkar ha lite olika åsikter när det kommer till docblock kommentarer, kan jag skriva om dem så att de båda verktygen blir glada och mina värden går upp? Hur viktigt 
            är det att lösa just sådana issues? Det får jag återkomma till i diskussionen.</p>
            <p>4. Minska komplexitet. Kan jag bryta ut delar av metoder eller skriva om saker så att de blir mindre komplexa och på så sätt nå bättre värden?</p>
            <p>5. Skriva fler kommentarer, ibland hör man att bra kod är så lätt att läsa att det inte behövs några kommentarer men hur ser phpmetrics på det? Jag ska testa att lägga till 
            fler kommentarer där det känns som att de kan bidra med förtydligande av koden så får vi se vilken effekt det får.</p>
            <h2 id="Resultat">Resultat</h2>
            <p><b>Före:</b></p>
            <p>Avg LCOM: 1.68, Avg cyclomatic complexity: 4.44, Avg bugs: 0.15, Avg defects: 0.45</p>
            <img src="{{ asset('img/classphpmetrics.jpg') }}" alt="class metrics before">
            <img src="{{ asset('img/scrutbefore.jpg') }}" alt="scrutinizer before improvements">
            <p><b>Efter:</b></p>
            <p>Avg LCOM: 1.68, Avg cyclomatic complexity: 4.44, Avg bugs: 0.15, Avg defects: 0.45.</p>
            <p>Issues tog jag ned från 21 till 3, jag ökade betyget med 0.03 och min coverage med 2 procentenheter. Då jag hade stor täckning av mina klasser sedan tidigare (som inte involverar 
            controllers) så fanns där inte så mycket täckning att öka. Men som syns nedan på phpmetrics så försökte jag istället göra testerna jag redan hade mer utförliga. Och jag gick från 91 assertions 
            till 113 assertions i testerna.</p>
            <img src="{{ asset('img/scrutImprovements.jpg') }}" alt="scrutinizer after improvements">
            <img src="{{ asset('img/scrutiafter.jpg') }}" alt="scrutinizer after improvements">
            <p>Att testa otestad kod ger snabbt resultat hos scrutinizer och visualiserar ännu tydligare hur CRAP score är relaterat bla. till tester. Det såg vi under tidigare kursmoment med 
            phpunit.</p>
            <img src="{{ asset('img/classmetricsafter.jpg') }}" alt="class metrics after">
            <p>Enligt phpmetrics ser det ut som om ingenting har förändrats, om något så fick jag 
            ytterligare en violation, eller varning. Där Scrutinizer tyckte om att jag arbetade med min DeckOfCards-klass så tyckte snarare phpmetrics att den blev lite större och då 
            mer trolig att innehålla buggar men jag utökade testningen av mina två mest komplexa klasser.</p>
            <p>Det faktum att jag löste dokumentationskonflikten mellan phpstan och scrutinizer gav ingen förbättring av koden ävem om det "löste" 14 issues. Det är ändå rimligt att 
            ett så litet problem inte påverkar betyget på kodbasen nämnvärt, intressant att veta. Inte heller lösningen av andra issues så som "unused code" och "best practice" ökade 
            poängen i scrutinizer, intressant.</p>
            <p>Något som däremot hade effekt var att minska komplexiteten i mina metoder där det var möjligt. Jag hittade framför allt två metoder som jag kunde bryta ut och göra lite mindre 
            vilket i sin tur minskade varje metods komplexitet och gav ett bättre betyg.</p>
            <p>Att lägga till fler kommentarer verkade inte påverka phpmetrics i någon större utsträckning, jag försökte både kommentera mindre och större klasser mer utförligt, både med docblock 
            kommentarer och med enklare kommentarer i koden men de verkade inte påverka phpmetrics.</p>
            <p>Att lägga till fler tester på de mer komplicerade klasserna tycks inte ha någon större effekt på hur phpmetrics bedömmer sannolikheten för buggar. Jag lade till 4 tester till DeckOfCards och 
            2 till TwentyOne men ingen av de nu mer utförligt testade klasserna gick ned i siffran för hur många buggar som förväntas i koden. Troligtvis har det alltså med klassernas inneboende 
            komplexitet att göra som leder till att phpmetrics tar fram en siffra för sannolikt antal buggar och därmed också uppmanar en att ha koll på sina tester.</p>
            <p>LCOM bör nog rimligtvis inte gå ned särskillt mycket då mina klasser i sig har bra och låga värden utan problemet när det kommer till coupling ligger i mina controllers 
            och dessa har jag inte valt att fokusera på, men jag tar definitivt med mig att de finns där och har förbättringspotential.</p>
            <h2 id="Diskussion">Diskussion</h2>
            <p><b>Resultaten</b></p>
            <p>Av resultaten att dömma så ser det inte ut som om särskillt mycket skett efter detta kursmoment men jag skulle ändå vilja lyfta fram något detta kursmoment verkligen lärt mig, 
            att siffrorna inte visar hela verkligheten. Nog för att det hade varit fruktbart både för kodens kvalitet och mätresultaten att ha testat mina controllers men även vad som 
            tycks vara ett blygsamt resultat gav mycket lärdom i vad de olika verktygen letar efter och vikten i att förstå vad de visar dig snarare än att stirra sig blind på siffrorna.</p>
            <p>Att bryta ut och göra metoder mindre gav positiva resultat, däremot påverkar ju detta inte klassens komplexitet i sig utan det klagar fortfarande mätverktygen på, 
            det skulle vid det här laget kräva för mycket arbete att lösa. Scrutinizer ger här bra tips på hur det skulle kunna lösas t.ex. att bryta ut metoder till en helt ny klass eller 
            genom att implementera ett interface. De båda tipsen har givetvis sina meriter och skulle troligen ge ett ännu bättre betyg men jag tycker metoderna passar bra där dom är och ser inte 
            riktigt hur jag skulle skriva vissa av dom som en egen klass. Ett interface hade varit intressant att ge sig på men det känns något stort i förhållande till uppgiftens storlek och de andra 
            förbättringar jag gav mig på. Vidare så ger tester och bredare kodtäckningen både lägre crap-score och kanske till följd också bättre betyg med scrutinizer.</p>
            <p><b>Kan man aktivt jobba med kodkvalitet och “clean code” på detta sättet?</b></p>
            <p>Det tycker jag definitivt att man kan göra, däremot är dess fruktbarhet en annan fråga. Hur många verktyg skall man använda sig av? Hur mycket används detta i arbetslivet? 
            Jag kan tänka mig att det varierar stort beroende på arbetsplats och uppdrag för även om det kan argumenteras att dessa verktyg hjälper oss skriva renare kod som potentiellt är 
            mer lättunderhållen och till större del är fri från buggar så bör det ju nämnas att detta tar tid. Så jag kan tänka mig att det är något som måste värderas och omvärderas beroende på 
            situation. Är det alltid bäst att ha en hel pipeline av test suites och valideringsverktyg eller är det ibland mer effektivt att "bara" använda sig utav linters och enhetstester? Ja ibland 
            finns det troligtvis varken tid eller resurser för mer än så. Nu sitter ju jag här med förmånen att ha ett helt kursmoment dedikerat till detta men det är inget jag tar för givet. En 
            annan aspekt jag tycker är värd att framhäva är ett begrepp som ofta nämns vilket är "technical debt", dvs en teknisk skuld som så att säga uppkommer genom att vi skriver kod som inte 
            är optimal, kanske är slarvig eller är komplex eller inte särskillt testad. Skulden uppstår då genom att det är snabbt och effektivt att skriva koden i stunden, förhoppningsvis fungerar 
            den också som den ska, men vad händer när det går fel och vi inte spenderat tid på att kvalitetssäkra koden? Ja då är det dags att betala tillbaka den skulden. Jag kan tänka mig scenarion där 
            man i efterhand ångrar att man inte spenderade mer tid på kvalitetssäkring av den kod som skrivs likt att använda sig av dessa verktyg, de blir på ett sätt en försäkring mot framtida problem 
            och kan ge mycket tillbaka i framtiden.</p>
            <p><b>Finns det fördelar och kanske nackdelar?</b></p>
            <p>Likt det jag beskrivit ovan så tycker jag att det både finns fördelar och nackdelar, jag skulle dock vilja påstå att fördelarna överväger nackdelarna. Därmed inte sagt att det 
            alltid är bäst att göra på ett visst sätt. Denna form utav kvalitetssäkring med olika verktyg är trots deras automatisering ändå rätt så tidskrävande så det får nog räknas till den stora 
            nackdelen, sedan ska man dessutom förstå vad verktygen säger dig och det kan ibland kännas lättare sagt än gjort. Men som sagt, fördelarna överväger nackdelarna enligt mig. Dessa verktyg 
            hjälper till att göra koden mer lättbegriplig och lättare att underhålla om man ger dem tid och tar till sig och förstår de synpunkter verktygen ger dig. Samma argument kan föras kring linters 
            eller kommentarer för den delen. Visst, det tar tid att skriva kommentarer och rätta fel som inte automatiskt rättas av verktyg som csfix men om vi ser till ett längre perspektiv så tror 
            jag definitivt att det är tid vi tjänar tillbaka med råge i framtiden om vi behöver komma tillbaka till vår kod eller om vi jobbar i ett projekt med andra.</p>
            <p><b>Ser du andra möjligheter att jobba mot “clean code”?</b></p>
            <p>Ja det skulle i så fall vara i en mer nedskalad form där man enbart använder sig utav ett verktyg, en annan kombination av verktyg eller inga alls för den delen. Det viktiga blir i 
            slutändan att det fungerar för alla inblandade, alla är nöjda och koden uppfyller de krav som finns på den. Verktygen är just det, vertyg, och vilka verktyg vi väljer att ta med i vår 
            verktygslåda varierar med person och projekt.</p>
            <p><b>Maslow's behovshierarki</b></p>
            <p>Jag kan inte hålla mig ifrån att nämna Maslow, som gammal sociologistudent, när han tas upp på föreläsningen med Mikael. Likt hur hans behovshierarki appliceras på oss människor och 
            våra behov, från basala till självförverkligande, så kan de lustigt nog även appliceras på programmering. Men det är inte enbart lustigt utan jag tycker att det är en givande tankeövning också, 
            vilka är de mest basala behoven när det kommer till min kod? till mitt skolarbete? till projektet på företaget? Kanske är det precis som pyramiden visar och som nämndes i föreläsningen 
            så att det först och främst gäller att sakerna fungerar och går att utveckla. Och först när vi löst detta och det finns tid och resurser till övers väljer vi att ta ett steg uppåt i pyramiden. 
            Och ju högre vi kommer desto snyggare, lättunderhållen, vältestad och genomtänkt blir koden. Jag tycker det är en intressant tankeövning och jag tar personligen med mig att det är trevligt 
            med bra skriven "clean code" som är lätt att underhålla, men framför allt är det nog viktigast att koden gör det den ska. Och om den gör det den ska så behöver jag kanske inte heller hänga 
            med huvudet över de blygsamma förbättringarna jag lyckades åstadkomma enligt mätverktygen.</p>
        </div>
    </article>
{% endblock %}